# G検定

## 第1章

レベル1：シンプルな制御プログラム

レベル2：古典的な人工知能（探索・推論・知識データ利用）

レベル3：機械学習

レベル4：Deep Learning

AI効果：わかってしまうと「それは単純な自動化」

人工知能とロボット：脳＝人工知能　考えるという目に見えないもの

1946年 電算機ENIAC（エニアック）ペンシルバニア

1956年ダートマス会議 ジョンマッカーシー ニューウェルサイモン

世界初の人工知能プログラミング（ロジック・セオリス 数学の定理）

1950～1960年 第一次AIブーム（探索と推論）

- トイプロブレム（おもちゃの問題）

1980年代 第二次AIブーム（知識の時代）

- エキスパートシステム

2010年～ 第三次AIブーム（機械学習・特徴表現）

- ビッグデータ、知識を定義する要素 特徴量、AlphaGo
- レイカールワイル シンギュラリティ

## 第2章

探索木

- 幅優先：ノード→隣のノード、最短ゴール、メモリ使う

- 深さ優先：行き止まりまで行く、メモリあまり使わない

プランニング

- 前提条件、行動、結果
- STRIPS（Stanford Research Institute Program Solve）
- 積み木の世界 SHRDLU テリー・ウィノグラード Cycプロジェクト

ボードゲーム

- オセロ：8＊8  10の60乗

- チェス：8＊8 10の120乗

- 将棋：9＊9 10の220乗

- 囲碁：19＊19 10の360

  ※水素 宇宙 10の80乗

Mini-Max法

- αカット：スコア最小 大きいノードをカット
- βカット：スコア最大 小さいノードをカット

人口無能（知識なし）

イライザ（ELIZA） 1964-1966 ジョゼフ・ワイゼンバウム

エキスパートシステム

- MYCIN スタンフォード バクテリア ルールベース
- DENDRAL 1960年代 有機化合物

- 知識獲得のボトルネック（専門家・ドキュメント・事例）

意味ネットワーク（Semantic network）

「概念」ラベル付きノード Is-a継承 Part-of部分

オントロジー：知識を体系化する方法論

全ての一般常識をコンピュータに取り込む（ダグラス・レナート）1984年

エキスパートシステムのための知識ベースの開発・保守にコストがかかる

ヘビーウェイトオントロジー：Cycプロジェクト

ライトウェイトオントロジー：ウェブマイニング、データマイニング

機械学習

1956年：ダートマス会議

1967年：k-mean法

1975年：遺伝的アルゴリズム

1990年：Web

1992年：サポートベクターマシン

1998年：Google

2000年：ビッグデータ

2011年：ワトソン ジョバディ

Deep Learning

1958年：単純パーセプトロン

1981年：バックプロパゲーション

2006年：Auto Encoder（自己符号）

2012年：ILSVRC トロント SuperVision トロント

2017年：AlphaGo

## 第3章

1969年 フレーム問題 ジョンマッカーシー ダニエル・デネット 

アランチューリング 「チューリングテスト」 中国の部屋

1966年 ELIZA

1980年 ジョンサール 強いAI、弱いAI

シンギュラリティ（技術的特異点） 人工知能が賢くなり無限に知能の高い存在を作る（2045年）

## 第4章

正則化

- L1 ラッソ回帰：一部のパラメータを0
- L2 リッジ回帰：パラメータの大きさに応じて0に近づける
- オーバーフィッティング対策

ランダムフォレスト：複数の決定木を作り多数決、アンサンブル学習

ブートストラップサンプリング

バギング：全体から一部のデータを用いて、複数のモデルを用いて学習する方法

ブースティング：一部のデータを繰り返し抽出、複数のモデルを学習する

逐次的にモデルを作成、時間がかかる

検証

ホールドアウト：訓練データとテストデータに分割する

k-分割交差検証：訓練データ・テストデータの分割を複数回行う

評価指標

混同行列

| 正解/予測 | イヌ | オオカミ |
| --------- | ---- | -------- |
| イヌ      | TP   | FN       |
| オオカミ  | FP   | TN       |

正解率＝(TP+TN)/全体

適合率＝TP/(TP+FP)

再現率＝TP/(TP+FN)

F値＝2＊PRECISION*RECALL/(PRECISION+RECALL)

## 第5章

ジェフリーヒントン

深層信念ネットワーク（Deep Belief Network）

オートエンコーダーに相当する層に制限付きボルツマンマシン

Google TPU（Tensor Processing Unit）

## 第6章

ドロップアウト：ランダムにニューロンをドロップアウトさせる（アンサンブル学習）

正規化：0～1

標準化：平均0、分散1

白色化：変数を無相関化

重みの初期値を工夫：シグモイド関数 Xavierの初期値、ReLU関数Heの初期値

バッチ正規化：各層において活性化関数をかける前に伝搬してきたデータを正規化する

オーバーフィッティングしにくくなる

CNN

S細胞：単純型細胞、パターン検出

C細胞：複雑型細胞、移動不変性

福島邦彦ネオコグニトロン

ヤン・ルカン：LeNet（畳み込み層、プーリング層）

